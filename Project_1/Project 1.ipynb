{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMS filter and ADALINE algorithm\n",
    "\n",
    "In this first project you will implement a Least Mean Square (LMS) error filter by using the Adaptive Linear Neuron (ADALINE) algorithm. This algorithm is a class of adaptive filter used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired and the actual signal). It is a stochastic gradient descent method in that the filter is only adapted based on the error at the current time. It was invented in 1960 by Stanford University professor Bernard Widrow and his first Ph.D. student, Ted Hoff.\n",
    "\n",
    "To fully understand the concepts of this filter I recommend you to watch the following lecture by professor Widrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" \\\\\\\n",
    "     src=\"https://www.youtube.com/embed/hc2Zj55j1zU\" \\\\\\\n",
    "     title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; \\\\\\\n",
    "     autoplay; clipboard-write; encrypted-media; \\\\\\\n",
    "     gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After watching professor Widrow's lecture, you can also take a look at [this paper](https://isl.stanford.edu/~widrow/papers/j1975thecomplex.pdf) to fully understand the LMS filter and ADALINE algorithm. A good summary on how the algorithm works is presented in section 2 of [this page](https://www.clear.rice.edu/elec422/1999/nsekila/LMSAlgorithm.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Implement a LMS filter and ADALINE algorithm to find the coefficients of a Digital Filter\n",
    "The main problem of this project is to implement the LMS filter and ADALINE algorithm to obtain the coefficients of a filter response $w[n]$ given the input $x[n]$ and the convolution $y[n] = x[n]*w[n]$. Since this is a supervised machine learning algorithm, you will train your model with a buffered version of the input $x[n]$ and compare your calculations with the expected outputs $y[n]$. After many iterations, you will notice that the coefficients tend to converge to the desired ones of the filter while reducing the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining the Algorithm\n",
    "In this first part you will have to create a block diagram of the LMS filter and ADALINE algorithm based on the information given to you in the previous section, to do so, you can use any available tool that you want to create your block diagram as a png image.\n",
    "\n",
    "[comment]: <> (Your image should be called algorithm.png and be saved in the folder Images.)\n",
    "<img src=\"Images/algorithm.png\" alt=\"Block Diagram\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create a buffer of streamed data\n",
    "To understand how data is processed by the filter you need to think about a buffer $Z$ that will be filled with some input data. First suppose that your buffer is of size 5 and is completely empty, for example full of zeros, and you have a vector $x$ also of size 5 with some data. Now let's see Figure 1 and understand what happens on every time step.\n",
    "\n",
    "<img src=\"Images/buffer.png\" alt=\"buffer\" width=\"300\"/>\n",
    "\n",
    "You can see that the data inside of $Z$ is moved and there are two special cases:\n",
    "1. Data inside $Z$ is fully loaded (green color).\n",
    "2. Data inside $Z$ is loaded and emptied (orange and green colors)\n",
    "\n",
    "With this image in mind, you can think that a filter processes a chunk of data given by the buffer on a specific time step instead of a recursive manner. This way you have a training data for a period of time that can be used in our LMS filter and ADALINE algorithm.\n",
    "\n",
    "Now it is your turn to create a function called `get_buffer` that generates a buffer matrix $Z$. This buffer matrix can be in fully loaded or a loaded and emptied form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buffer(x, buffer_size=5, form='fl'):\n",
    "    \"\"\"\n",
    "    Function that generates a buffer matrix with a fully loaded or loaded and emptied form.\n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "    buffer_size (int): Size of buffer.\n",
    "    form (string): String that represent the form of the Z matrix. \n",
    "                   Can be 'fl' for fully loaded or 'lae' for loaded and emptied.\n",
    "                   By default fully loaded is selected.\n",
    "  \n",
    "    Returns: \n",
    "    Z (numpy array): Matrix with a fully loaded or loaded and emptied form.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your `get_buffer` function with the following code and check if it matches your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.arange(0,16)\n",
    "\n",
    "print(\"Fully Loaded Form\")\n",
    "print(get_buffer(test, buffer_size=8, form='fl'), \"\\n\")\n",
    "print(\"Loaded and Emptied Form\")\n",
    "print(get_buffer(test, buffer_size=4, form='lae'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement convolution\n",
    "Now that you have a way to represent a vector $x$ as a buffer matrix $Z$ you will have to use it to implement a convolution as a matrix product between $Z$ and $w$, where $w$ is a vector of the coefficients of a filter response.\n",
    "\n",
    "Your results should match the usage of the `numpy` function `convolve` as follows:\n",
    "1. When using $Z_{lae}$ results should match `np.convolve(x, w, mode='full')`.\n",
    "2. When using $Z_{fl}$ results should match `np.convolve(x, w, mode='full')[0:-w.shape[0]+1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-5, 1, -3, 2, 4, 0, 1, -7, 9, 3, -5, -6, 8, 4, 3, 0, 1, -4])\n",
    "w = np.array([7, -3, 1, 4, -9, -2])\n",
    "\n",
    "# Add your first test here: compare using Z_lae\n",
    "Z_lae = None\n",
    "conv_lae = None\n",
    "numpy_conv_lae = None\n",
    "\n",
    "print(\"Convolution using Z_lea \\n {}\".format(conv_lae))\n",
    "print(\"Convolution using numpy_lea \\n {}\".format(numpy_conv_lae))\n",
    "print(\"Comparison using Z_lea and numpy is same?: {} \\n\".format((conv_lae==numpy_conv_lae).all()))\n",
    "\n",
    "# Add your second test here: compare using Z_fl\n",
    "Z_fl = None\n",
    "conv_fl = None\n",
    "numpy_conv_fl = None\n",
    "\n",
    "print(\"Convolution using Z_fl \\n {}\".format(conv_fl))\n",
    "print(\"Convolution using numpy_fl \\n {}\".format(numpy_conv_fl))\n",
    "print(\"Comparison using Z_fl and numpy is same?: {}\".format((conv_fl==numpy_conv_fl).all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Implement LMS filter and ADALINE algorithm\n",
    "Now it is time to implement your LMS filter and ADALINE algorithm, in order to do so you need to create a function called `adaline_filter` which takes as arguments the following values:\n",
    "* `X` which is a matrix in fully loaded or loaded and emptied form.\n",
    "* `w` an initial vector for the estimated filter.\n",
    "* `y_hat` the expected output vector for the filter, sometimes called ground truth.\n",
    "* `alpha` is the learning rate or convergence factor (step size).\n",
    "* `epochs` is the number of iterations.\n",
    "\n",
    "As outputs you will have:\n",
    "* `w` which is an updated version of the initial input vector $w$.\n",
    "* `loss` is an array vector that stores the mean square error loss function, MSE, for every epoch or iteration, you can read more about the MSE loss function [here](https://en.wikipedia.org/wiki/Mean_squared_error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaline_filter(X, w, y_hat, alpha=0.0005, epochs=100):\n",
    "    \"\"\"\n",
    "    Function that generates a buffer matrix with a fully loaded or loaded and emptied form.\n",
    "    Parameters: \n",
    "    X (numpy array): Matrix in fully loaded or loaded and emptied form.\n",
    "    w (numpy array): Initial vector for the estimated filter.\n",
    "    y_hat (numpy array): Expected output vector for the filter, sometimes called ground truth.\n",
    "    alpha (float): Learning rate or convergence factor (step size).\n",
    "    epochs (int): Number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "    w (numpy array): Updated version of the initial input vector w.\n",
    "    loss (numpy array): Array vector that stores the mean square error loss function for every\n",
    "                        epoch or iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your algorithm an input vector $x$ and the ground truth vector `y_hat` are given, you will have to estimate your $w$ vector using the `adaline_filter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, -3, 4, -6, 2, 4, -1, 7, 4, 8, 6, -1, 0, 3, -9, -7])\n",
    "y_hat = np.array([4, 10, -7, 9, -23, 13, -4, 32, 12, 21, 58, 21, 18, -12, 9, -15, -45, -32, 26, 3, -14])\n",
    "w = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "# Add your code here\n",
    "Z = None\n",
    "\n",
    "# Add your code here\n",
    "w_est, loss_mse = None, None\n",
    "\n",
    "print(\"Estimated w values are: {}\".format(w_est))\n",
    "\n",
    "plt.plot(loss_mse, label=\"Learning = 0.0005\")\n",
    "plt.title(\"MSE Loss vs. Iteration\");\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Plot Results with Different Learning Rates\n",
    "In this part you will compare and plot your results by using three different learning rates `alpha` and `epochs = 200` in the same figure. For this you need to use the following values:\n",
    "* `alpha = 0.002`\n",
    "* `alpha = 0.005`\n",
    "* `alpha = 0.0006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "w1, loss1 = None, None\n",
    "w2, loss2 = None, None\n",
    "w3, loss3 = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss1, label=\"Learning = 0.0002\");\n",
    "plt.plot(loss2, label=\"Learning = 0.0005\");\n",
    "plt.plot(loss3, label=\"Learning = 0.00006\");\n",
    "plt.title(\"MSE Loss vs. Iteration\");\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Apply your LMS filter and ADALINE algorithm\n",
    "In this part you will use your algorithm to find the coefficients of a filter based on the expected results of the output. \n",
    "For this problem an input signal with three different tones of $30, 50$ and $150Hz$ is sampled at $800Hz$, added to the input signal there's also noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Generation\n",
    "np.random.seed(123)\n",
    "\n",
    "fc_1 = 50\n",
    "fc_2 = 150\n",
    "fc_3 = 30\n",
    "\n",
    "# Add your code here\n",
    "fs = None\n",
    "Ts = None\n",
    "\n",
    "t = np.arange(0,0.10,Ts)\n",
    "signal_1 = np.sin(2*np.pi*fc_1*t)\n",
    "signal_2 = np.sin(2*np.pi*fc_2*t)\n",
    "signal_3 = np.sin(2*np.pi*fc_3*t)\n",
    "\n",
    "noise = np.random.rand(signal_1.shape[0])\n",
    "\n",
    "# Input signal\n",
    "x = signal_1 + signal_2 + signal_3 + noise\n",
    "\n",
    "# Expected output signal\n",
    "y_hat = signal_1 + signal_3\n",
    "\n",
    "# Initial filter coefficients\n",
    "w = np.zeros(51)\n",
    "\n",
    "plt.plot(x);\n",
    "plt.stem(x, use_line_collection=True);\n",
    "plt.title(\"Input Signal\");\n",
    "plt.xlabel(\"sample\")\n",
    "plt.ylabel(\"amplitude\")\n",
    "plt.grid(\"on\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the coefficients of the filter\n",
    "Z = None\n",
    "w, loss = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of your filter against the expected signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = np.convolve(x, w, mode='full')\n",
    "plt.plot(conv, label=\"Estimate\");\n",
    "plt.plot(signal_1 + signal_3, label=\"Expected\");\n",
    "plt.title(\"Comparison between convolutions\");\n",
    "plt.xlabel(\"sample\")\n",
    "plt.ylabel(\"amplitude\")\n",
    "plt.legend()\n",
    "plt.grid(\"on\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Conclusions\n",
    "You are at the end of this project, to finalize, write three conclusions of what you have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
