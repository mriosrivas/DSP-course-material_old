{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics, Probability and Noise\n",
    "Statistics and probability are used in Digital Signal Processing to characterize signals and the processes that generate them. For example, a primary use of DSP is to reduce interference, noise, and other undesirable components in acquired data. These may be an inherent part of the signal being measured, arise from imperfections in the data acquisition system, or be introduced as an unavoidable byproduct of some DSP operation. Statistics and probability allow these disruptive features to be measured and classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Signal and Graph Terminology\n",
    "A signal is a description of how one parameter depends on another parameter. When both parameters have a continuous range, we call this signals **continuous**, on the other hand, when both parameters have a quantized range we call these signals **discrete**. In this course we will focus on discrete signals.\n",
    "\n",
    "To keep things general, we will simply label the horizontal axis: **sample number**. If this were a continuous signal, another label would have to be used, such as: time, distance, x, etc.\n",
    "\n",
    "The type of parameter on the horizontal axis is the domain of the signal. Some common domains are: **time**, **frequency** and **spatial domain**. When we just refer to **sample number** we are usually referring to time domain.\n",
    "\n",
    "The variable, $N$, is widely used in DSP to represent the total number of samples in a signal. Two notations for assigning sample numbers are commonly used. In the first notation, the sample indexes run from $1$ to $N$. In the second notation, the sample indexes run from $0$ to $N-1$. We will use the second notation since *Python* indexes start by $0$. *MATLAB* would be a good example of using the first notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean and Standard Deviation\n",
    "\n",
    "The **mean** is the average value of a signal:\n",
    "\n",
    "$$\\mu = \\frac{1}{N}\\sum_{i=0}^{N-1}x[i]$$\n",
    "\n",
    "The **standard deviation** measure how far the signal fluctuates from the mean:\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{N-1}\\sum_{i=0}^{N-1}(x[i]-\\mu)^2}$$\n",
    "\n",
    "It is of special interest because it fits well with the physics of how signals operate. For example, when random noise signals combine in an electronic circuit, the resultant noise is equal to the combined power of the individual signals, not their combined amplitude.\n",
    "\n",
    "The standard deviation, $\\sigma$, is a measure of **how far** the signal fluctuates from the mean. The variance, $\\sigma^2$, represents the **power** of this fluctuation.\n",
    "\n",
    "By definition, the standard deviation only measures the AC portion of a signal, while the **rms** value measures both the AC and DC components. If a signal has no DC component, its rms value is identical to its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Common import common_plots\n",
    "cplots = common_plots.Plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following part you will develop three functions called `mean`, `variance` and `std` which correspond to the mean, variance and standard deviation equations explained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    \"\"\" \n",
    "    Function that calculates the mean of an input signal x.\n",
    "  \n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "  \n",
    "    Returns: \n",
    "    float: Returns mean of a input signal x\n",
    "    \"\"\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\" \n",
    "    Function that calculates the variance of an input signal x.\n",
    "  \n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "  \n",
    "    Returns: \n",
    "    float: Returns variance of a input signal x\n",
    "    \"\"\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def std(x):\n",
    "    \"\"\" \n",
    "    Function that calculates the standard deviation of an input signal x.\n",
    "  \n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "  \n",
    "    Returns: \n",
    "    float: Returns standard deviation of a input signal x\n",
    "    \"\"\"\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code you will use a sin function and calculate the mean, standard deviation and variance of that signal and compare with the knowledge you have about this type of signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,4*np.pi,100)\n",
    "x = (np.sin(t)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cplots.plot_single(x.T, style='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_mean = mean(x)\n",
    "print('The mean of the signal is: {}'.format(signal_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_std = std(x)\n",
    "print('The standard deviation of the signal is: {}'.format(signal_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_var = variance(x)\n",
    "print('The variance of the signal is: {}'.format(signal_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running Statistics\n",
    "This method of calculating the mean and standard deviation is adequate for many applications; however, it has two limitations:\n",
    "\n",
    "1. If the mean is much larger than the standard deviation, the equation involves subtracting two numbers that are very close in value. This can result in excessive round-off error in the calculations, \n",
    "2. It is often desirable to recalculate the mean and standard deviation as new samples are acquired and added to the signal. The previous equation, for calculating the standard deviation, requires that all of the samples be involved in each new calculation. This is a very inefficient use of computational power and memory.\n",
    "\n",
    "A solution to these problems can be found by using the following equation:\n",
    "\n",
    "$$\\sigma^2=\\frac{1}{N-1}\\left[ \\sum_{i=0}^{N-1}x[i]^2 - \\frac{1}{N}\\left(\\sum_{i=0}^{N-1}x[i]\\right){^2} \\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement a function called `running_statistics` which calculates the mean, variance and standard deviation of a signal using the previous equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_statistics(x):\n",
    "    \"\"\" \n",
    "    Function that calculates the mean, variance and standard deviation \n",
    "    of an input signal x in a recurrent manner.\n",
    "  \n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "  \n",
    "    Returns: \n",
    "    mean (float): Returns mean of a input signal x\n",
    "    var (float): Returns variance of a input signal x\n",
    "    std (float): Returns standard deviation of a input signal x\n",
    "    \"\"\"\n",
    "           \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_signal_mean, rs_signal_var, rs_signal_std = running_statistics(x)\n",
    "\n",
    "print('The mean of the signal is: {}'.format(rs_signal_mean))\n",
    "print('The standard deviation of the signal is: {}'.format(rs_signal_std))\n",
    "print('The variance of the signal is: {}'.format(rs_signal_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this new implementation can perform calculations of the mean, variance and standard deviation, the update problem is still present. To solve this issue we can create a class called `Running_Statistics` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Running_Statistics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.acc = 0\n",
    "        self.acc_of_squares = 0\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.var = 0\n",
    "        self.std = 0\n",
    "        return\n",
    "        \n",
    "    def update(self, x):\n",
    "        \"\"\" \n",
    "        Function that updates the number of iterations, accumulator and sum of\n",
    "        squares accumulator.\n",
    "\n",
    "        Parameters: \n",
    "        x (numpy array): One dimensional array representing a input signal sample.\n",
    "\n",
    "        Returns: \n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def calc(self, x):\n",
    "        \"\"\" \n",
    "        Function that calculates the mean, variance and standard deviation \n",
    "        of an input signal sample.\n",
    "\n",
    "        Parameters: \n",
    "        x (numpy array): One dimensional array representing a input signal sample.\n",
    "\n",
    "        Returns: \n",
    "        mean (float): Returns mean of a input signal sample x\n",
    "        var (float): Returns variance of a input signal sample x\n",
    "        std (float): Returns standard deviation of a input signal sample x\n",
    "        \"\"\"\n",
    "        \n",
    "        return None, None, None\n",
    "    \n",
    "    \n",
    "    def run(self,x):\n",
    "        \"\"\" \n",
    "        Function that calculates first updates the number of iterations, accumulator \n",
    "        and sum of squares accumulator, and then calculates the mean, variance and \n",
    "        standard deviation of an input signal sample.\n",
    "\n",
    "        Parameters: \n",
    "        x (numpy array): One dimensional array representing a input signal sample.\n",
    "\n",
    "        Returns: \n",
    "        mean (float): Returns mean of a input signal sample x\n",
    "        var (float): Returns variance of a input signal sample x\n",
    "        std (float): Returns standard deviation of a input signal sample x\n",
    "        \"\"\"\n",
    "        \n",
    "        return None        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the statistics of a signal as data is being updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x.shape[0]\n",
    "statistics = Running_Statistics()\n",
    "\n",
    "mean_acc = []\n",
    "var_acc = []\n",
    "std_acc = []\n",
    "\n",
    "for i in range(N):\n",
    "    mean_, var_, std_ = statistics.run(x[i][0])\n",
    "    mean_acc.append(mean_)\n",
    "    var_acc.append(var_)\n",
    "    std_acc.append(std_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the behavior of the statistics as it runs by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_acc, label='mean')\n",
    "plt.plot(var_acc, label='variance')\n",
    "plt.plot(std_acc, label='standard deviation')\n",
    "plt.title('Running Statistics Behavior')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, the mean describes what is being measured, while the standard deviation represents noise and other interference. In these cases, the standard deviation is not important in itself, but only in comparison to the mean. This gives rise to two concepts:\n",
    "\n",
    "* **Signal to Noise Ratio (SNR)**: $$SNR=\\frac{\\mu}{\\sigma}$$\n",
    "* **Coefficient of variation (CV)**: $$CV = \\frac{\\sigma}{\\mu}\\times 100$$\n",
    "\n",
    "Better data means a higher value for the SNR and a lower value for the CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Signal vs. Underlying Process\n",
    "Statistics is the science of interpreting numerical data, such as acquired signals. In comparison, probability is used in DSP to understand the processes that generate signals.\n",
    "\n",
    "The probabilities of the underlying process are constant, but the statistics of the acquired signal change each time the experiment is repeated. This random irregularity found in actual data is called by such names as: statistical variation, statistical fluctuation, and statistical noise.\n",
    "\n",
    "This presents a bit of a dilemma. When you see the terms: mean and standard deviation, how do you know if the author is referring to the statistics of an actual signal, or the probabilities of the underlying process that created the signal? Unfortunately, the only way you can tell is by the context.\n",
    "\n",
    "For random signals, the typical error between the mean of the $N$ points, and the mean of the underlying process, is given by:\n",
    "\n",
    "$$error = \\frac{\\sigma}{\\sqrt{N}}$$\n",
    "\n",
    "If $N$ is small, the statistical noise in the calculated mean will be very large. In other words, you do not have access to enough data to properly characterize the process. The larger the value of $N$, the smaller the expected error will become. A milestone in probability theory, the **Strong Law of Large Numbers**, guarantees that the error becomes zero as $N$ approaches infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation Problem\n",
    "Before you can calculate the standard deviation, you need to already know the mean, $\\mu$. However, you don't know the mean of the underlying process, only the mean of the $N$ point signal, which contains an error due to statistical noise. This error tends to **reduce the calculated value of the standard deviation**. To compensate for this, $N$ is replaced by $N-1$. If $N$ is large, the difference doesn't matter. If $N$ is small, this replacement provides a more accurate estimate of the standard deviation of the underlying process.\n",
    "\n",
    "In other words, we calculated an estimate of the standard deviation of the underlying process. If we divided by $N$ in the equation, it would provide the standard deviation of the acquired signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Histogram and Probability Mass Function (PMF)\n",
    "A histogram is an approximate representation of the distribution of numerical data. To construct a histogram, the first step is to \"bin\" or divide the entire range of values into a series of intervals and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins or intervals must be adjacent, and are often, but not required to be, of equal size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will implement a function called `histogram` which calculates the histogram of a given signal using a ceiling rounding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(x, bins=100):\n",
    "    \"\"\" \n",
    "    Function that calculates the histogram distribution of an input signal x.\n",
    "\n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "    bins (int): Integer value representing the number of bins to use. Default 100.\n",
    "\n",
    "    Returns: \n",
    "    numpy array: Returns histogram distribution of inputs signal x divided\n",
    "    by the number of given bins.\n",
    "    \"\"\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your `histogram` function and plot your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 1000\n",
    "hist = histogram(x,bins)\n",
    "plt.bar(np.linspace(-1,1,bins),hist.flatten(), align='edge', width=0.005, label='{} bins'.format(bins))\n",
    "plt.title('Histogram of x[n]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion that the acquired signal is a noisy version of the underlying process is very important; so important that some of the concepts are given different names. The histogram is what is formed from an acquired signal. The corresponding curve for the underlying process is called the probability mass function (PMF). A histogram is always calculated using a finite number of samples, while the PMF is what would be obtained with an infinite number of samples. The PMF can be estimated (inferred) from the histogram, or it may be deduced by some mathematical technique, such as the *kernel* density estimation.\n",
    "\n",
    "\n",
    "The key to understanding these concepts rests in the units of the vertical axis. The vertical axis of the histogram is the number of times that a particular value occurs in the signal. The vertical axis of the PMF contains similar information, except expressed on a fractional basis. In other words, each value in the histogram is divided by the total number of samples to approximate the PMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Density Estimation\n",
    "In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzenâ€“Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.\n",
    "\n",
    "The idea behind the KDE is to find a density function, $\\hat{f}$ such that\n",
    "\n",
    "$$\\widehat{f}_h(x) = \\frac{1}{n}\\sum_{i=1}^n K_h (x - x_i) = \\frac{1}{nh} \\sum_{i=1}^n K\\Big(\\frac{x-x_i}{h}\\Big)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $K$ is the kernel function to use, in this case, a Gaussian distribution $$K(u) = \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-u^2}{2}}$$\n",
    "\n",
    "\n",
    "* $h>0$ is a smoothing parameter, called the bandwidth, which by rule of thumb can be\n",
    "\n",
    "$$h = \\left(\\frac{4}{3N}\\right)^{1/5}\\sigma$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a kernel density estimation function called `kde_estimation` that calculates a density function of a given input data. Use the auxiliary functions `phi`, `bandwidth`, and `kernel` to implement the Gaussian distribution, bandwidth, and kernel functions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    \"\"\" \n",
    "    Function that calculates the Gaussian distribution of an input signal x.\n",
    "\n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "\n",
    "    Returns: \n",
    "    numpy array: Returns Gaussian distribution of a given signal x.\n",
    "    \"\"\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def bandwidth(x):\n",
    "    \"\"\" \n",
    "    Function that calculates the bandwidth of a kernel density estimation\n",
    "    based on Silverman's Rule of Thumb.\n",
    "\n",
    "    Parameters: \n",
    "    x (numpy array): Array of numbers representing the input signal.\n",
    "\n",
    "    Returns: \n",
    "    float: Bandwidth of a kernel density estimation\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "def kernel(xi, x, h):\n",
    "    \"\"\" \n",
    "    Function that implements the kernel density function estimation for a\n",
    "    single input xi.\n",
    "\n",
    "    Parameters: \n",
    "    xi (numpy array): One dimensional array to use for estimating the kernel.\n",
    "    x (numpy array): Array representing a input signal.\n",
    "\n",
    "    Returns: \n",
    "    float: Bandwidth of a kernel density estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def kde_estimation(x, x_lin=np.linspace(-2,2,1000).reshape(-1,1)):\n",
    "    \"\"\" \n",
    "    Function that implements the kernel density function estimation of a given\n",
    "    input signal x.\n",
    "\n",
    "    Parameters: \n",
    "    x (numpy array): Array representing a input signal.\n",
    "    x_lin (numpy array): Array representing range of values used by the kde.\n",
    "\n",
    "    Returns: \n",
    "    numpy array: Array that represents the kde estimation based on a Gaussian\n",
    "    distribution and Silverman's Rule of Thumb bandwidth.\n",
    "    \"\"\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the PMF of the sine signal $x[n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 100\n",
    "hist = histogram(x,bins)\n",
    "hist_freq = hist.reshape(-1)/x.shape[0]\n",
    "hist_norm = hist_freq/hist_freq.max()\n",
    "\n",
    "x_lin=np.linspace(-2,2,bins).reshape(-1,1)\n",
    "pmf = kde_estimation(x, x_lin);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(0,1.1)\n",
    "plt.bar(np.linspace(-1,1,bins),hist_norm, align='edge', width=0.05, alpha=0.5)\n",
    "plt.plot(x_lin, pmf, c='orange', label='developed kde')\n",
    "plt.title('Histogram and PMF of x[n]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your `estimation` function with the `kde_scipy` function implemented in *SciPy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def kde_scipy(x, x_lin, bandwidth=0.2, **kwargs):\n",
    "    \"\"\"\n",
    "    Kernel Density Estimation using Scipy\n",
    "    \n",
    "    Parameters: \n",
    "    x (numpy array): Array representing a input signal.\n",
    "    x_lin (numpy array): Array representing range of values used by the kde.\n",
    "\n",
    "    Returns: \n",
    "    numpy array: Array that represents the kde estimation based on a Gaussian\n",
    "    distribution and Silverman's Rule of Thumb bandwidth.\n",
    "    \"\"\"\n",
    "    kde = gaussian_kde(x, bw_method='silverman', **kwargs)\n",
    "    return kde.evaluate(x_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(-2,2,100)\n",
    "\n",
    "plt.bar(np.linspace(-1,1,bins),hist_norm, align='edge', width=0.05, alpha=0.5);\n",
    "plt.plot(x_grid,kde_scipy(x.T, x_grid), linestyle='-', label='scipy kde')\n",
    "plt.plot(x_lin,pmf, linestyle='-.', label='developed kde')\n",
    "plt.title('Histogram and PMF of x[n]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise for Statistics Probability and Noise\n",
    "Create a class called `Statistics`that has the following methods: `mean`, `variance`, `std`, `running_statistics`, `histogram`, and `kde_estimation`. All methods, except `running_statistics` will perform on the whole signal. `running_statistics` must perform one sample at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "http://www.dspguide.com/ch2.htm\n",
    "\n",
    "https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "\n",
    "https://seaborn.pydata.org/tutorial/distributions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
